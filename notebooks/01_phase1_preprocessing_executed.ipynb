{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Dialogue Preprocessing\n",
    "\n",
    "This notebook processes dialogue documents and extracts structured conversation data.\n",
    "\n",
    "## Steps:\n",
    "1. Extract text from Word/PDF documents\n",
    "2. Normalize speakers (learner/bot)\n",
    "3. Generate turn lists\n",
    "4. Save as JSON files (W1_T1.json, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T03:58:56.733905Z",
     "iopub.status.busy": "2025-11-23T03:58:56.732378Z",
     "iopub.status.idle": "2025-11-23T03:58:57.114278Z",
     "shell.execute_reply": "2025-11-23T03:58:57.111747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\jasmi\\OneDrive\\Documents\\GitHub\\Designing-Data-Responsive-Language-Learning-Environments-through-Conversation-Practice-with-AI\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Add scripts directory to path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root / 'scripts'))\n",
    "\n",
    "from document_extractor import extract_text, save_extracted_text\n",
    "from dialogue_parser import DialogueParser\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract Text from Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T03:58:57.183526Z",
     "iopub.status.busy": "2025-11-23T03:58:57.182524Z",
     "iopub.status.idle": "2025-11-23T03:58:57.196069Z",
     "shell.execute_reply": "2025-11-23T03:58:57.193044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents to process:\n",
      "  week1: #18. Week1.docx - ✓\n",
      "  week2: #12. Week2.docx - ✓\n",
      "  week3: #16. Week3.docx - ✗\n",
      "  week4: #14. Week4.pdf - ✓\n"
     ]
    }
   ],
   "source": [
    "# Define document paths\n",
    "raw_data_dir = project_root / 'data' / 'raw'\n",
    "extracted_text_dir = project_root / 'data' / 'extracted_text'\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "\n",
    "# Document mapping\n",
    "documents = {\n",
    "    'week1': {\n",
    "        'file': raw_data_dir / '#18. Week1.docx',\n",
    "        'format': 'week1_week2'\n",
    "    },\n",
    "    'week2': {\n",
    "        'file': raw_data_dir / '#12. Week2.docx',\n",
    "        'format': 'week1_week2'\n",
    "    },\n",
    "    'week3': {\n",
    "        'file': raw_data_dir / '#16. Week3.docx',\n",
    "        'format': 'week3'\n",
    "    },\n",
    "    'week4': {\n",
    "        'file': raw_data_dir / '#14. Week4.pdf',\n",
    "        'format': 'week4'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Documents to process:\")\n",
    "for week, info in documents.items():\n",
    "    exists = info['file'].exists()\n",
    "    print(f\"  {week}: {info['file'].name} - {'✓' if exists else '✗'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T03:58:57.205613Z",
     "iopub.status.busy": "2025-11-23T03:58:57.205613Z",
     "iopub.status.idle": "2025-11-23T03:58:58.378470Z",
     "shell.execute_reply": "2025-11-23T03:58:58.378470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting text from week1...\n",
      "Saved extracted text to: C:\\Users\\jasmi\\OneDrive\\Documents\\GitHub\\Designing-Data-Responsive-Language-Learning-Environments-through-Conversation-Practice-with-AI\\data\\extracted_text\\week1_extracted.txt\n",
      "  Extracted 21865 characters\n",
      "  Preview (first 200 chars): Task one\n",
      "\n",
      "You said:\n",
      "“Hi, I would like to order a coffee today!”\n",
      "English Conversational Partner said:\n",
      "Sure! What kind of coffee would you like today — something like an espresso, latte, or cappuccino?\n",
      "...\n",
      "\n",
      "Extracting text from week2...\n",
      "Saved extracted text to: C:\\Users\\jasmi\\OneDrive\\Documents\\GitHub\\Designing-Data-Responsive-Language-Learning-Environments-through-Conversation-Practice-with-AI\\data\\extracted_text\\week2_extracted.txt\n",
      "  Extracted 16568 characters\n",
      "  Preview (first 200 chars): Week 2 – Task 1\n",
      "You said：\n",
      "“Some more, I'm visiting a medical clinic in person, and you will help me to book the service.”\n",
      "00:14\n",
      "English Conversational Partner said：\n",
      "Absolutely, I'd be happy to help wi...\n",
      "Warning: C:\\Users\\jasmi\\OneDrive\\Documents\\GitHub\\Designing-Data-Responsive-Language-Learning-Environments-through-Conversation-Practice-with-AI\\data\\raw\\#16. Week3.docx not found, skipping...\n",
      "\n",
      "Extracting text from week4...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted text with color information\n",
      "Saved extracted text to: C:\\Users\\jasmi\\OneDrive\\Documents\\GitHub\\Designing-Data-Responsive-Language-Learning-Environments-through-Conversation-Practice-with-AI\\data\\extracted_text\\week4_extracted.txt\n",
      "  Extracted 13806 characters\n",
      "  Preview (first 200 chars): Week4\n",
      "Task 1: Discussing Study or Career Plans with an Advisor\n",
      "Hi, I want to study overseas in the future. Could you tell me how to prepare now?\n",
      "Absolutely! Studying overseas is exciting. To get start...\n"
     ]
    }
   ],
   "source": [
    "# Extract text from all documents\n",
    "extracted_texts = {}\n",
    "week4_color_data = None  # Store color data for Week4\n",
    "\n",
    "from document_extractor import extract_text_with_colors_from_pdf\n",
    "\n",
    "for week, info in documents.items():\n",
    "    if not info['file'].exists():\n",
    "        print(f\"Warning: {info['file']} not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nExtracting text from {week}...\")\n",
    "    try:\n",
    "        # Special handling for Week4 to extract color information\n",
    "        if week == 'week4':\n",
    "            try:\n",
    "                week4_color_data = extract_text_with_colors_from_pdf(str(info['file']))\n",
    "                # Also get plain text\n",
    "                text = extract_text(str(info['file']))\n",
    "                extracted_texts[week] = text\n",
    "                print(f\"  Extracted text with color information\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not extract colors, using plain text: {e}\")\n",
    "                text = extract_text(str(info['file']))\n",
    "                extracted_texts[week] = text\n",
    "        else:\n",
    "            text = extract_text(str(info['file']))\n",
    "            extracted_texts[week] = text\n",
    "        \n",
    "        # Save extracted text\n",
    "        output_file = extracted_text_dir / f\"{week}_extracted.txt\"\n",
    "        save_extracted_text(text, str(output_file))\n",
    "        \n",
    "        print(f\"  Extracted {len(text)} characters\")\n",
    "        print(f\"  Preview (first 200 chars): {text[:200]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error extracting {week}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 & 3: Parse Dialogues and Generate Turn Lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T03:58:58.381846Z",
     "iopub.status.busy": "2025-11-23T03:58:58.381846Z",
     "iopub.status.idle": "2025-11-23T03:58:58.398783Z",
     "shell.execute_reply": "2025-11-23T03:58:58.397768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing WEEK1\n",
      "============================================================\n",
      "\n",
      "Parsed 127 turns\n",
      "\n",
      "First 3 turns:\n",
      "  Turn 1 (learner): “Hi, I would like to order a coffee today!”...\n",
      "  Turn 2 (bot): Sure! What kind of coffee would you like today — something like an espresso, lat...\n",
      "  Turn 3 (learner): “Hi, could I please get a matcha latte?”...\n",
      "\n",
      "============================================================\n",
      "Processing WEEK2\n",
      "============================================================\n",
      "\n",
      "Parsed 0 turns\n",
      "\n",
      "First 3 turns:\n",
      "\n",
      "============================================================\n",
      "Processing WEEK4\n",
      "============================================================\n",
      "\n",
      "Parsed 8 turns\n",
      "\n",
      "First 3 turns:\n",
      "  Turn 1 (bot): Week4Task 1: Discussing Study or Career Plans with an Advisor Hi, I want to stud...\n",
      "  Turn 2 (bot): Do you have any recommendation for studying education in the UK?Sure! There are ...\n",
      "  Turn 3 (bot): goals and celebrate those little achievements along the way. Also, exploring dif...\n"
     ]
    }
   ],
   "source": [
    "# Initialize parser\n",
    "parser = DialogueParser()\n",
    "\n",
    "# Process each week\n",
    "all_dialogues = {}\n",
    "\n",
    "for week, info in documents.items():\n",
    "    if week not in extracted_texts:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {week.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    text = extracted_texts[week]\n",
    "    \n",
    "    # Parse based on format\n",
    "    if info['format'] == 'week1_week2':\n",
    "        turns = parser.parse_week1_week2(text)\n",
    "    elif info['format'] == 'week3':\n",
    "        turns = parser.parse_week3(text)\n",
    "    elif info['format'] == 'week4':\n",
    "        # Use color data if available\n",
    "        turns = parser.parse_week4_pdf(text, color_data=week4_color_data)\n",
    "    else:\n",
    "        print(f\"Unknown format for {week}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nParsed {len(turns)} turns\")\n",
    "    \n",
    "    # Preview first few turns\n",
    "    print(\"\\nFirst 3 turns:\")\n",
    "    for turn in turns[:3]:\n",
    "        print(f\"  Turn {turn['turn']} ({turn['speaker']}): {turn['text'][:80]}...\")\n",
    "    \n",
    "    all_dialogues[week] = turns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: Split Dialogues into Tasks (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T03:58:58.402355Z",
     "iopub.status.busy": "2025-11-23T03:58:58.402355Z",
     "iopub.status.idle": "2025-11-23T03:58:58.426954Z",
     "shell.execute_reply": "2025-11-23T03:58:58.426954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dialogue to: C:\\Users\\jasmi\\OneDrive\\Documents\\GitHub\\Designing-Data-Responsive-Language-Learning-Environments-through-Conversation-Practice-with-AI\\data\\processed\\W1_T1.json (127 turns)\n",
      "\n",
      "WEEK1: Saved as single task W1_T1.json\n",
      "No turns found for week2, skipping...\n",
      "Saved dialogue to: C:\\Users\\jasmi\\OneDrive\\Documents\\GitHub\\Designing-Data-Responsive-Language-Learning-Environments-through-Conversation-Practice-with-AI\\data\\processed\\W4_T1.json (8 turns)\n",
      "\n",
      "WEEK4: Saved as single task W4_T1.json\n",
      "\n",
      "============================================================\n",
      "Summary: Saved 2 dialogue files\n",
      "============================================================\n",
      "  W1_T1.json\n",
      "  W4_T1.json\n"
     ]
    }
   ],
   "source": [
    "# Split dialogues into tasks and save\n",
    "saved_files = []\n",
    "\n",
    "for week, turns in all_dialogues.items():\n",
    "    if not turns:\n",
    "        print(f\"No turns found for {week}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    week_num = week.replace('week', '')\n",
    "    text = extracted_texts[week]\n",
    "    info = documents[week]  # Get document format info\n",
    "    \n",
    "    # Try to split into tasks\n",
    "    tasks = parser.split_into_tasks(text, int(week_num))\n",
    "    \n",
    "    if len(tasks) > 1:\n",
    "        print(f\"\\n{week.upper()}: Found {len(tasks)} tasks\")\n",
    "        # Parse each task separately\n",
    "        for task_idx, (task_name, task_text) in enumerate(tasks, 1):\n",
    "            # Parse the task text\n",
    "            if info['format'] == 'week1_week2':\n",
    "                task_turns = parser.parse_week1_week2(task_text)\n",
    "            elif info['format'] == 'week3':\n",
    "                task_turns = parser.parse_week3(task_text)\n",
    "            elif info['format'] == 'week4':\n",
    "                task_turns = parser.parse_week4_pdf(task_text, color_data=week4_color_data)\n",
    "            else:\n",
    "                task_turns = []\n",
    "            \n",
    "            if task_turns:\n",
    "                # Renumber turns starting from 1\n",
    "                for i, turn in enumerate(task_turns, 1):\n",
    "                    turn['turn'] = i\n",
    "                \n",
    "                output_file = processed_dir / f\"W{week_num}_T{task_idx}.json\"\n",
    "                parser.save_dialogue_json(task_turns, str(output_file))\n",
    "                saved_files.append(output_file)\n",
    "                print(f\"  Saved {output_file.name} with {len(task_turns)} turns\")\n",
    "    else:\n",
    "        # Single task - save all turns\n",
    "        output_file = processed_dir / f\"W{week_num}_T1.json\"\n",
    "        parser.save_dialogue_json(turns, str(output_file))\n",
    "        saved_files.append(output_file)\n",
    "        print(f\"\\n{week.upper()}: Saved as single task {output_file.name}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Summary: Saved {len(saved_files)} dialogue files\")\n",
    "print(f\"{'='*60}\")\n",
    "for f in saved_files:\n",
    "    print(f\"  {f.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Preview Generated JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T03:58:58.429276Z",
     "iopub.status.busy": "2025-11-23T03:58:58.429276Z",
     "iopub.status.idle": "2025-11-23T03:58:58.450221Z",
     "shell.execute_reply": "2025-11-23T03:58:58.448198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of W1_T1.json:\n",
      "------------------------------------------------------------\n",
      "Total turns: 127\n",
      "\n",
      "First 5 turns:\n",
      "\n",
      "Turn 1 - learner:\n",
      "  “Hi, I would like to order a coffee today!”\n",
      "\n",
      "Turn 2 - bot:\n",
      "  Sure! What kind of coffee would you like today — something like an espresso, latte, or cappuccino?\n",
      "\n",
      "Turn 3 - learner:\n",
      "  “Hi, could I please get a matcha latte?”\n",
      "\n",
      "Turn 4 - bot:\n",
      "  Absolutely, a matcha latte sounds great! Would you like it hot or iced?\n",
      "\n",
      "Turn 5 - learner:\n",
      "  “HOTAS.”\n",
      "\n",
      "... and 122 more turns\n"
     ]
    }
   ],
   "source": [
    "# Load and preview one of the generated files\n",
    "if saved_files:\n",
    "    sample_file = saved_files[0]\n",
    "    print(f\"Preview of {sample_file.name}:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Total turns: {len(data)}\")\n",
    "    print(f\"\\nFirst 5 turns:\")\n",
    "    for turn in data[:5]:\n",
    "        print(f\"\\nTurn {turn['turn']} - {turn['speaker']}:\")\n",
    "        print(f\"  {turn['text']}\")\n",
    "    \n",
    "    if len(data) > 5:\n",
    "        print(f\"\\n... and {len(data) - 5} more turns\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
