"""
Repair detection using Gemini API.
Implements Phase 2: LLM Repair Detection.
"""
import os
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import google.generativeai as genai

# Load environment variables
load_dotenv()


# System prompt for repair detection
REPAIR_DETECTION_SYSTEM_PROMPT = """You are an expert analyst of learner–AI dialogues in second language learning.

Your goal is to detect and label **repair sequences** in a dialogue between a learner and an AI chatbot, following a specific theoretical codebook.

You MUST:

- Carefully read the entire dialogue.
- Identify all episodes where *communication trouble* occurs AND someone attempts to fix it.
- For each such episode, create a structured JSON object with:
  - where the repair happens (turn indices),
  - who initiated it (LI vs BI),
  - how it ended (R, U-A, or U-P),
  - what caused the trouble (trigger),
  - a short explanation grounding your coding in the dialogue.

If there are **no repair sequences**, you MUST return an empty JSON array: `[]`.

===============================
THEORETICAL DEFINITIONS
===============================

A. REPAIR SEQUENCES (what you are looking for)

A repair sequence is a stretch of dialogue where:

1) There is **communication trouble**:
   - misunderstanding,
   - lack of understanding,
   - unclear reference,
   - incorrect recognition (e.g. ASR / mishearing),
   - confusion about task, terms, or context.

AND

2) At least one party **tries to fix** the problem, e.g.:
   - asking for clarification,
   - repeating or rephrasing,
   - checking understanding,
   - correcting themselves or the other.

CRITICAL: You should treat the entire cluster of turns related to the same problem as **one repair event**, UNLESS the sequence contains multiple distinct misunderstandings from different sources. In that case, split them into separate repairs.

Examples of trouble:
- "Sorry, I didn't catch you."
- "What does X mean?"
- "I don't understand."
- "Could you repeat that?"
- Bot gives an irrelevant or incoherent answer.
- Learner's utterance is so unclear that the bot has to guess or ask again.
- Learner uses incorrect word (e.g., "bookie" instead of "booking") and bot corrects it.
- Learner's phrasing is fragmented/unclear (e.g., broken grammar causing confusion).
- Phonetic/lexical errors (e.g., "whitelist" misheard as "wine list").
- Learner makes contextual error (e.g., "7 a.m." for "tonight" → should be "7 p.m.").

WHAT IS **NOT** A REPAIR SEQUENCE:

❌ **Normal conversation flow** - Do NOT mark as repairs:
   - Normal question-answer pairs where there is no misunderstanding
   - Bot making recommendations or suggestions (e.g., suggesting "oat milk" when learner says "alt-milk" - this is normal service, not a repair)
   - Confirmation questions ("right?", "OK?") that don't follow actual communication trouble
   - Stylistic reformulations that don't change meaning (e.g., "can we sit outside?" → "can I request outdoor seating?" - same meaning)
   - Normal semantic interpretations (e.g., interpreting "buy the window" as "window table" - this is normal understanding, not a repair)
   
CONCRETE EXAMPLES OF WHAT NOT TO MARK (from real dialogues):
   - ❌ "alt-milk" → bot suggests "oat milk" = NOT a repair (normal service recommendation)
   - ❌ "buy the window" → bot says "window table" = NOT a repair (normal semantic interpretation)
   - ❌ "can we sit outside?" → "can I request outdoor seating?" = NOT a repair (stylistic reformulation, same meaning)
   - ❌ Learner says "right?" after smooth conversation = NOT a repair (confirmation check, no trouble)
   - ❌ Bot makes typo/omission but learner doesn't respond = NOT a repair (no repair attempt)
   - ❌ Learner says "right?" to confirm a complex order change that was already understood = NOT a repair (routine confirmation, not fixing trouble)
   - ❌ Learner asks "right?" to confirm a complex order change that was already understood = NOT a repair (routine confirmation, not fixing trouble)

❌ **Bot errors without repair attempts** - Do NOT mark as repairs:
   - Bot makes a mistake or omission (e.g., missing time, typo)
   - BUT learner does not respond to it or try to fix it
   - No repair attempt = NOT a repair sequence

❌ **Self-corrections without trouble** - Do NOT mark as repairs:
   - Learner self-corrects, BUT the original utterance was clear and caused no misunderstanding
   - Only count self-corrections if the original caused actual communication trouble
   
✅ **Self-corrections WITH trouble** - DO mark as repairs:
   - Learner starts to say something, hesitates ("um, no"), then corrects to something different
   - This shows the learner recognized their own error/confusion and fixed it
   - Example: "a slice of, um, no, a banana bread" = self-correction repair (LI)

B. DIMENSION 1 – INITIATION (who signals trouble first?)

You must classify **initiation** as:

- **LI (Learner-Initiated)**  
  The learner signals trouble first.

  This includes:
  - Explicit signals:
    - "I don't understand."
    - "Can you repeat that?"
    - "What do you mean by X?"
  - Implicit signals:
    - Hesitations, incomplete turns, or self-corrections that clearly show confusion.
    - Learner reformulates their own previous utterance to fix a problem.

- **BI (Bot-Initiated)**  
  The bot signals trouble first.

  This includes:
  - Explicit signals:
    - "Sorry, I didn't catch that, could you rephrase?"
    - "Did you mean X?"
    - "I'm not sure I understand."
  - Implicit signals (ONLY when there is clear evidence of trouble):
    - Bot produces an irrelevant or incoherent response that clearly shows misinterpretation of the learner's previous turn.
    - Bot repeatedly misinterprets the learner's intent so the learner has to clarify several times.
    - Learner's utterance is genuinely unintelligible/unclear (e.g., "HOTAS" for "hot") and bot must interpret it.
    - Learner uses incorrect word (e.g., "bookie" for "booking") and bot corrects/rephrases it.
    - Learner makes contextual/logical error (e.g., "7 a.m." for "tonight") and bot corrects it.
    - Learner's phrasing is fragmented/unclear causing bot to interpret/clarify (e.g., broken grammar about allergies).

CRITICAL DISTINCTION for BI:
   - ✅ Bot interpreting genuinely unclear/unintelligible speech (e.g., "HOTAS" → "hot") = BI
   - ✅ Bot correcting a clear error (e.g., "7 a.m." for "tonight" → "7 p.m.") = BI
   - ✅ Bot correcting lexical error (e.g., "bookie" → "booking/reservation") = BI
   - ✅ Bot interpreting fragmented/unclear phrasing (e.g., broken grammar about allergies) = BI
   - ✅ Bot correcting phonetic/lexical confusion (e.g., "whitelist" → "wine list") = BI
   - ❌ Bot making a normal recommendation or suggestion ≠ BI (e.g., learner says "alt-milk", bot suggests "oat milk" - this is normal service, not a repair)
   - ❌ Bot doing normal semantic interpretation ≠ BI (e.g., "buy the window" → "window table" - this is normal understanding, not a repair)

Important:
- Choose LI vs BI based on **who first reacts to the trouble**, not who finally "explains more".
- If the learner's output is unclear but the **bot** is the first to surface the problem (e.g. "I didn't catch that"), that is BI.
- If the bot's answer is confusing and the **learner** then asks for clarification, that is LI.
- If the bot makes a normal interpretation or suggestion without evidence of trouble, it is NOT a repair.

C. DIMENSION 2 – RESOLUTION (how does it end?)

You must classify **resolution** as:

- **R (Resolved)**  
  The trouble is successfully fixed, and the dialogue continues with clear mutual understanding.

  Indicators:
  - Follow-up turns are coherent and relevant.
  - Learner's question is answered clearly.
  - Learner uses the clarified information correctly.
  - The task or topic progresses without obvious confusion on the same issue.
  - Learner accepts/clarifies and the conversation moves forward smoothly.
  
  IMPORTANT: 
  - If a bot misunderstanding is followed by learner clarification that the bot then confirms, the resolution is R (resolved), NOT U-P (unresolved-persists). The fact that clarification was needed doesn't mean it's unresolved - if it ends with mutual understanding, it's R.
  - When repairs are SPLIT (e.g., bot misunderstanding in turns 57-58, then learner clarification in turns 59-64), each repair should be evaluated independently:
    * The first repair (bot misunderstanding) can be R if the bot's interpretation allows the conversation to continue, even if later clarified
    * OR it can be evaluated based on whether the immediate response shows understanding
  - Generally, if a repair sequence ends with mutual understanding (even after clarification), mark it as R.

- **U-A (Unresolved–Abandoned)**  
  The trouble is *not* fixed, and the participants move on or drop it.

  Indicators:
  - Learner or bot changes topic without resolving the issue.
  - Learner says "OK" or "never mind" but does not show real understanding.
  - The conversation continues but the original problem is left hanging.

- **U-P (Unresolved–Persists)**  
  The trouble is not fixed and keeps causing problems, despite multiple attempts.

  Indicators:
  - 2 or more repair attempts on the same issue.
  - Learner explicitly expresses ongoing confusion or frustration.
  - Bot keeps giving irrelevant/wrong responses to the same issue.

When you decide R vs U-A vs U-P, look at what happens in the **subsequent turns**, not just the immediate response.

D. TRIGGER (what caused the trouble?)

For each repair sequence, assign a short descriptive "trigger" string.  
Examples (non-exhaustive):

- "vocabulary – did not understand word/phrase"
- "pronunciation/ASR – misrecognition of learner speech"
- "task misunderstanding – unclear what to do in the task"
- "bot misunderstanding – irrelevant or off-topic answer"
- "ambiguous question – learner confused by bot's question"
- "self-correction – learner corrects own previous utterance"
- "other – [short description]"

Be as specific as you reasonably can from the dialogue.

===============================
OUTPUT REQUIREMENTS
===============================

You MUST output a single JSON array.  
Each element in the array must be an object with this exact schema:

- `dialogue_id` (string) – supplied from the input.
- `repair_id` (integer) – 1, 2, 3, … in order of appearance in the dialogue.
- `turn_indices` (array of integers) – list of turn numbers involved in this repair sequence.  
  Include:
    - the turn where trouble is signaled,
    - any clarifying question or explanation,
    - the immediate resolution attempt(s),
    - the turn that shows the issue is resolved (e.g., learner's acceptance like "OK" or "thank you").
  
  IMPORTANT: Include ALL turns that are part of the repair sequence, including the resolution confirmation turn.
- `initiation` (string) – one of `"LI"` or `"BI"`.
- `resolution` (string) – one of `"R"`, `"U-A"`, `"U-P"`.
- `trigger` (string) – short description of trouble source (e.g., "vocabulary – didn't understand 'up-to-date'").
- `evidence_summary` (string) – 1–3 sentences explaining:
    - what the trouble was,
    - who initiated the repair,
    - why you coded the resolution as R / U-A / U-P.

Example structure (not real data):

[
  {
    "dialogue_id": "W2_T1_S18",
    "repair_id": 1,
    "turn_indices": [5, 6, 7],
    "initiation": "LI",
    "resolution": "R",
    "trigger": "pronunciation/ASR – learner's word misrecognized",
    "evidence_summary": "Learner says 'HOTAS', which is unclear. Bot asks clarifying question and offers an interpretation. Learner then confirms and the order proceeds smoothly, so the issue is resolved."
  }
]

If you detect **no repair sequences**, return:

[]

(without any additional text).

===============================
DECISION STRATEGY
===============================

When analysing a dialogue:

1. First, scan all turns and **mark candidate trouble spots**:
   - learner explicitly says they don't understand,
   - learner asks for repetition or clarification,
   - bot explicitly says it didn't understand,
   - bot's response is clearly incoherent or off-topic,
   - repeated questions or self-corrections on the same issue.

2. For each candidate spot, decide:
   - Is there genuine communication trouble here?
   - Is there an attempt to fix it (by learner or bot)?
   - What is the smallest continuous span of turns that covers the trouble and its attempted resolution?

3. Only then assign:
   - LI vs BI,
   - R vs U-A vs U-P,
   - trigger type,
   - evidence summary grounded in the actual turns.

4. Avoid double-counting AND splitting correctly:
   - Group multiple closely linked attempts about the **same underlying issue** into one repair event.
   - **SPLIT repairs** if they contain multiple distinct misunderstandings from different sources:
     * Example: Bot misunderstanding (turns 57-58: bot misinterprets "without milk" as suggesting "oat milk") + separate learner self-correction (turns 59-64: learner clarifies to "almond milk") = TWO repairs
     * The first is BI (bot-initiated misunderstanding), the second is LI (learner-initiated clarification)
     * These are TWO separate issues: (1) bot's misinterpretation, (2) learner's need to clarify their actual preference
     * Example: Bot error + separate learner clarification on different issue = TWO repairs
   - Separate them if they clearly refer to different issues or arise from different trouble sources.
   - When in doubt, ask: "Are these two separate problems, or one problem with multiple clarification attempts?" If separate problems → split.
   - CRITICAL: If a bot misunderstanding is followed by learner clarification that addresses a DIFFERENT aspect (e.g., bot misunderstands "no milk" → learner clarifies "almond milk"), these are TWO repairs.

5. Be conservative - avoid false positives:
   - Do NOT mark normal question–answer pairs as repairs if there was no trouble.
   - Do NOT mark confirmation questions ("right?", "OK?") as repairs unless they follow actual communication trouble.
   - Do NOT mark bot recommendations/suggestions as repairs (e.g., suggesting "oat milk" when learner says "alt-milk").
   - Do NOT mark stylistic reformulations as repairs if meaning doesn't change (e.g., "can we sit outside?" → "can I request outdoor seating?" - same meaning).
   - Do NOT mark bot errors/omissions as repairs if no one tries to fix them (e.g., bot says "at p.m." but learner doesn't respond to it).
   - Do NOT mark normal semantic interpretations as repairs (e.g., "buy the window" → "window table" - this is normal understanding, not trouble).
   - Do NOT invent trouble that is not supported by the text.
   - Only mark as repair if there is BOTH trouble AND an attempt to fix it.

You MUST follow these instructions strictly.
Return ONLY the final JSON array, with no extra commentary.

IMPORTANT: You MUST return a complete, valid JSON array. Do not truncate or leave the JSON incomplete. If there are no repairs, return an empty array: [].

Make sure every repair object has ALL required fields:
- dialogue_id
- repair_id
- turn_indices (array)
- initiation ("LI" or "BI")
- resolution ("R", "U-A", or "U-P")
- trigger (string)
- evidence_summary (string)

Return the complete JSON array now:"""


def get_gemini_model():
    """Get the best available Gemini model."""
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY not found in environment variables")
    
    genai.configure(api_key=api_key)
    
    # List available models
    models = genai.list_models()
    available_models = [m.name for m in models if 'generateContent' in m.supported_generation_methods]
    
    # Prefer free models: flash models are typically free tier
    preferred_models = ['gemini-2.5-flash', 'gemini-1.5-flash', 'gemini-1.5-pro']
    model_name = None
    
    for pref in preferred_models:
        matching = [m for m in available_models if pref in m]
        if matching:
            model_name = matching[0]
            break
    
    # Fallback to first available model
    if not model_name and available_models:
        model_name = available_models[0]
    elif not model_name:
        model_name = 'models/gemini-2.5-flash'
    
    # Extract just the model name (without 'models/' prefix)
    model_short_name = model_name.split('/')[-1] if '/' in model_name else model_name
    
    return genai.GenerativeModel(model_short_name)


def create_user_prompt(dialogue_data: Dict[str, Any]) -> str:
    """Create the user prompt with dialogue JSON."""
    dialogue_id = dialogue_data.get('dialogue_id', 'UNKNOWN')
    
    prompt = f"""You are given a single learner–AI dialogue in JSON format.

- `student_id` is the learner ID.
- `dialogue_id` uniquely identifies this dialogue.
- `turns` is a list of turn objects, each with:
    - `turn` (integer): turn index (1, 2, 3, …)
    - `speaker` (string): "learner" or "bot"
    - `text` (string): the utterance text

Your job is to detect and label all repair sequences in this dialogue, following the definitions and output schema from the system prompt.

Use the `dialogue_id` exactly as given below.

Here is the dialogue JSON:

```json
{json.dumps(dialogue_data, ensure_ascii=False, indent=2)}
```

Return the JSON array of repair annotations only."""
    
    return prompt


def extract_json_from_response(response_text: str) -> List[Dict[str, Any]]:
    """Extract JSON array from Gemini response, handling markdown code blocks and incomplete responses."""
    # Remove markdown code blocks if present
    original_text = response_text.strip()
    
    # First, try to extract from markdown code blocks
    # Find content between ```json and ``` markers
    code_block_match = re.search(r'```(?:json)?\s*(.*?)\s*```', original_text, re.DOTALL)
    if code_block_match:
        response_text = code_block_match.group(1).strip()
        # If it starts with [, it's likely a JSON array
        if response_text.startswith('['):
            # Use bracket counting to find the complete array
            bracket_count = 0
            for i, char in enumerate(response_text):
                if char == '[':
                    bracket_count += 1
                elif char == ']':
                    bracket_count -= 1
                    if bracket_count == 0:
                        response_text = response_text[:i+1]
                        break
    else:
        # Try to find JSON array directly (greedy match)
        json_match = re.search(r'(\[.*\])', original_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(1)
        else:
            response_text = original_text
    
    # If no match, try to find incomplete JSON and fix it
    if not response_text.startswith('['):
        # Look for start of array
        start_idx = response_text.find('[')
        if start_idx >= 0:
            response_text = response_text[start_idx:]
    
    # Try to fix incomplete JSON by finding the last complete object
    if response_text.startswith('['):
        # Count brackets to find where array might be complete
        bracket_count = 0
        last_complete_idx = -1
        
        for i, char in enumerate(response_text):
            if char == '[':
                bracket_count += 1
            elif char == ']':
                bracket_count -= 1
                if bracket_count == 0:
                    last_complete_idx = i
                    break
        
        # If we found a complete array, use it
        if last_complete_idx > 0:
            response_text = response_text[:last_complete_idx + 1]
        else:
            # Try to close incomplete JSON manually
            # Find last complete object
            objects = []
            current_obj = ""
            brace_count = 0
            
            for char in response_text:
                current_obj += char
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        objects.append(current_obj)
                        current_obj = ""
            
            # If we have at least one complete object, reconstruct array
            if objects:
                response_text = '[' + ','.join(objects) + ']'
    
    # Parse JSON
    try:
        repairs = json.loads(response_text)
        if not isinstance(repairs, list):
            return []
        return repairs
    except json.JSONDecodeError as e:
        # Try one more time with more aggressive fixing
        try:
            # Remove any trailing incomplete content
            if response_text.count('[') > response_text.count(']'):
                # Find last complete object and close array
                last_brace = response_text.rfind('}')
                if last_brace > 0:
                    response_text = response_text[:last_brace + 1] + ']'
                    repairs = json.loads(response_text)
                    if isinstance(repairs, list):
                        return repairs
        except:
            pass
        
        print(f"Warning: Failed to parse JSON from response: {e}")
        print(f"Response text (first 1000 chars): {response_text[:1000]}")
        return []


def detect_repairs(dialogue_data: Dict[str, Any], model=None) -> List[Dict[str, Any]]:
    """
    Detect repair sequences in a dialogue using Gemini API.
    
    Args:
        dialogue_data: Dialogue JSON with student_id, dialogue_id, and turns
        model: Optional Gemini model instance (will create one if not provided)
    
    Returns:
        List of repair annotation dictionaries
    """
    if model is None:
        model = get_gemini_model()
    
    # Create user prompt
    user_prompt = create_user_prompt(dialogue_data)
    
    # Generate response with generation config to ensure complete output
    try:
        generation_config = {
            "temperature": 0.1,  # Lower temperature for more consistent output
            "max_output_tokens": 8192,  # Ensure enough tokens for complete JSON
        }
        
        response = model.generate_content(
            REPAIR_DETECTION_SYSTEM_PROMPT + "\n\n" + user_prompt,
            generation_config=generation_config
        )
        
        response_text = response.text
        
        # Extract JSON from response
        repairs = extract_json_from_response(response_text)
        
        return repairs
        
    except Exception as e:
        print(f"Error calling Gemini API: {e}")
        return []


def validate_repair_annotation(repair: Dict[str, Any], dialogue_id: str) -> bool:
    """Validate a repair annotation against the schema."""
    required_fields = ['repair_id', 'turn_indices', 'initiation', 'resolution', 'trigger', 'evidence_summary']
    
    for field in required_fields:
        if field not in repair:
            print(f"Warning: Missing required field '{field}' in repair annotation")
            return False
    
    # Validate dialogue_id matches
    if repair.get('dialogue_id') != dialogue_id:
        repair['dialogue_id'] = dialogue_id  # Fix it
    
    # Validate initiation
    if repair['initiation'] not in ['LI', 'BI']:
        print(f"Warning: Invalid initiation value: {repair['initiation']}")
        return False
    
    # Validate resolution
    if repair['resolution'] not in ['R', 'U-A', 'U-P']:
        print(f"Warning: Invalid resolution value: {repair['resolution']}")
        return False
    
    # Validate turn_indices is a list
    if not isinstance(repair['turn_indices'], list):
        print(f"Warning: turn_indices must be a list")
        return False
    
    return True


def save_repair_annotations(repairs: List[Dict[str, Any]], output_path: Path) -> None:
    """Save repair annotations to a JSON file."""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(repairs, f, ensure_ascii=False, indent=2)
    
    print(f"  [OK] Saved {len(repairs)} repair annotations to: {output_path}")

